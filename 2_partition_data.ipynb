{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['big.feather',\n",
       " 'rwerwe.txt',\n",
       " 'test_1050001_1120000.csv',\n",
       " 'test_1050001_1120000.feather',\n",
       " 'test_1120001_1190000.csv',\n",
       " 'test_1120001_1190000.feather',\n",
       " 'test_1190001_1260000.csv',\n",
       " 'test_1190001_1260000.feather',\n",
       " 'test_1260001_1330000.csv',\n",
       " 'test_1260001_1330000.feather',\n",
       " 'test_1330001_1400000.csv',\n",
       " 'test_1330001_1400000.feather',\n",
       " 'test_1400001_1470000.csv',\n",
       " 'test_1400001_1470000.feather',\n",
       " 'test_140001_210000.csv',\n",
       " 'test_140001_210000.feather',\n",
       " 'test_1470001_1540000.csv',\n",
       " 'test_1470001_1540000.feather',\n",
       " 'test_1540001_1610000.csv',\n",
       " 'test_1540001_1610000.feather',\n",
       " 'test_1610001_1680000.csv',\n",
       " 'test_1610001_1680000.feather',\n",
       " 'test_1680001_1750000.csv',\n",
       " 'test_1680001_1750000.feather',\n",
       " 'test_1750001_1820000.csv',\n",
       " 'test_1750001_1820000.feather',\n",
       " 'test_1820001_1890000.csv',\n",
       " 'test_1820001_1890000.feather',\n",
       " 'test_1890001_1960000.csv',\n",
       " 'test_1890001_1960000.feather',\n",
       " 'test_1960001_2030000.csv',\n",
       " 'test_1960001_2030000.feather',\n",
       " 'test_1_70000.csv',\n",
       " 'test_1_70000.feather',\n",
       " 'test_2030001_2100000.csv',\n",
       " 'test_2030001_2100000.feather',\n",
       " 'test_2100001_2170000.csv',\n",
       " 'test_2100001_2170000.feather',\n",
       " 'test_210001_280000.csv',\n",
       " 'test_210001_280000.feather',\n",
       " 'test_2170001_2240000.csv',\n",
       " 'test_2170001_2240000.feather',\n",
       " 'test_2240001_2310000.csv',\n",
       " 'test_2240001_2310000.feather',\n",
       " 'test_2310001_2380000.csv',\n",
       " 'test_2310001_2380000.feather',\n",
       " 'test_2380001_2450000.csv',\n",
       " 'test_2380001_2450000.feather',\n",
       " 'test_2450001_2520000.csv',\n",
       " 'test_2450001_2520000.feather',\n",
       " 'test_2520001_2590000.csv',\n",
       " 'test_2520001_2590000.feather',\n",
       " 'test_2590001_2660000.csv',\n",
       " 'test_2590001_2660000.feather',\n",
       " 'test_2660001_2730000.csv',\n",
       " 'test_2660001_2730000.feather',\n",
       " 'test_2730001_2800000.csv',\n",
       " 'test_2730001_2800000.feather',\n",
       " 'test_2800001_2870000.csv',\n",
       " 'test_2800001_2870000.feather',\n",
       " 'test_280001_350000.csv',\n",
       " 'test_280001_350000.feather',\n",
       " 'test_2870001_2940000.csv',\n",
       " 'test_2870001_2940000.feather',\n",
       " 'test_2940001_3010000.csv',\n",
       " 'test_2940001_3010000.feather',\n",
       " 'test_3010001_3080000.csv',\n",
       " 'test_3010001_3080000.feather',\n",
       " 'test_3080001_3150000.csv',\n",
       " 'test_3080001_3150000.feather',\n",
       " 'test_3150001_3220000.csv',\n",
       " 'test_3150001_3220000.feather',\n",
       " 'test_3220001_3290000.csv',\n",
       " 'test_3220001_3290000.feather',\n",
       " 'test_3290001_3360000.csv',\n",
       " 'test_3290001_3360000.feather',\n",
       " 'test_3360001_3430000.csv',\n",
       " 'test_3360001_3430000.feather',\n",
       " 'test_3430001_3500000.csv',\n",
       " 'test_3430001_3500000.feather',\n",
       " 'test_3500001_3570000.csv',\n",
       " 'test_3500001_3570000.feather',\n",
       " 'test_350001_420000.csv',\n",
       " 'test_350001_420000.feather',\n",
       " 'test_3570001_3640000.csv',\n",
       " 'test_3570001_3640000.feather',\n",
       " 'test_3640001_3710000.csv',\n",
       " 'test_3640001_3710000.feather',\n",
       " 'test_3710001_3780000.csv',\n",
       " 'test_3710001_3780000.feather',\n",
       " 'test_3780001_3850000.csv',\n",
       " 'test_3780001_3850000.feather',\n",
       " 'test_3850001_3920000.csv',\n",
       " 'test_3850001_3920000.feather',\n",
       " 'test_3920001_3990000.csv',\n",
       " 'test_3920001_3990000.feather',\n",
       " 'test_3990001_4060000.csv',\n",
       " 'test_3990001_4060000.feather',\n",
       " 'test_4060001_4130000.csv',\n",
       " 'test_4060001_4130000.feather',\n",
       " 'test_4130001_4200000.csv',\n",
       " 'test_4130001_4200000.feather',\n",
       " 'test_4200001_4270000.csv',\n",
       " 'test_4200001_4270000.feather',\n",
       " 'test_420001_490000.csv',\n",
       " 'test_420001_490000.feather',\n",
       " 'test_4270001_4340000.csv',\n",
       " 'test_4270001_4340000.feather',\n",
       " 'test_4340001_4410000.csv',\n",
       " 'test_4340001_4410000.feather',\n",
       " 'test_4410001_4480000.csv',\n",
       " 'test_4410001_4480000.feather',\n",
       " 'test_4480001_4550000.csv',\n",
       " 'test_4480001_4550000.feather',\n",
       " 'test_4550001_4620000.csv',\n",
       " 'test_4550001_4620000.feather',\n",
       " 'test_4620001_4690000.csv',\n",
       " 'test_4620001_4690000.feather',\n",
       " 'test_4690001_4760000.csv',\n",
       " 'test_4690001_4760000.feather',\n",
       " 'test_4760001_4830000.csv',\n",
       " 'test_4760001_4830000.feather',\n",
       " 'test_4830001_4900000.csv',\n",
       " 'test_4830001_4900000.feather',\n",
       " 'test_4900001_4970000.csv',\n",
       " 'test_4900001_4970000.feather',\n",
       " 'test_490001_560000.csv',\n",
       " 'test_490001_560000.feather',\n",
       " 'test_4970001_5040000.csv',\n",
       " 'test_4970001_5040000.feather',\n",
       " 'test_560001_630000.csv',\n",
       " 'test_560001_630000.feather',\n",
       " 'test_630001_700000.csv',\n",
       " 'test_630001_700000.feather',\n",
       " 'test_700001_770000.csv',\n",
       " 'test_700001_770000.feather',\n",
       " 'test_70001_140000.csv',\n",
       " 'test_70001_140000.feather',\n",
       " 'test_770001_840000.csv',\n",
       " 'test_770001_840000.feather',\n",
       " 'test_840001_910000.csv',\n",
       " 'test_840001_910000.feather',\n",
       " 'test_910001_980000.csv',\n",
       " 'test_910001_980000.feather',\n",
       " 'test_980001_1050000.csv',\n",
       " 'test_980001_1050000.feather']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n  = 50000\n",
    "df = pd.DataFrame({\"range\":range(1,50001)  } )\n",
    "df.to_csv('test_num.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning using CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test')]\n",
    "for file in files:\n",
    "    os.remove('./data/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [17:29, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049.610423395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_el = [0]\n",
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test') & x.lower().endswith('.csv') ]\n",
    "\n",
    "for file in files:\n",
    "    el = file.split('.')[0].split('test_')[-1].split(\"_\") \n",
    "    for e in el:\n",
    "        try:\n",
    "            new_el.append(int(e))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "saved = max(new_el)\n",
    " \n",
    "saved\n",
    "\n",
    "df_list = [] # list to hold the batch dataframe\n",
    "start = time.perf_counter()\n",
    "\n",
    "chunk = 70000\n",
    "counter = saved//chunk\n",
    "# print(saved,counter)\n",
    "try:\n",
    "    if counter ==0:\n",
    "        dfs = pd.read_csv(\"pickle_5Mx143.csv\", header=0,   chunksize=chunk)\n",
    " \n",
    "\n",
    "    else:\n",
    "        dfs = pd.read_csv(\"pickle_5Mx143.csv\", skiprows=list(range(1,saved+1)),header=0,   chunksize=chunk)\n",
    "    \n",
    "    \n",
    "    for count, df_chunk in tqdm(enumerate((dfs))):\n",
    "   \n",
    "  \n",
    "            try:\n",
    "                # Alternatively, append the chunk to list and merge all\n",
    "                rows  = str((count+counter)  *chunk+1) +\"_\" +str((count+counter)*chunk+chunk) \n",
    "#                 print(rows)\n",
    "                df_chunk.reset_index(drop=True,inplace=True)\n",
    "\n",
    "                df_chunk.to_csv(\"./data/test_{}.csv\".format(rows))\n",
    "            except:\n",
    "                print('pass')\n",
    "\n",
    "            \n",
    "except:\n",
    "    print('Data is at the end')\n",
    "end = time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning using Feather\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [02:35,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.07560259099955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_el = [0]\n",
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test') & x.lower().endswith('.feather') ]\n",
    "\n",
    "for file in files:\n",
    "    el = file.split('.')[0].split('test_')[-1].split(\"_\") \n",
    "    for e in el:\n",
    "        try:\n",
    "            new_el.append(int(e))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "saved = max(new_el)\n",
    " \n",
    "saved\n",
    "\n",
    "df_list = [] # list to hold the batch dataframe\n",
    "start = time.perf_counter()\n",
    "\n",
    "chunk = 70000\n",
    "counter = saved//chunk\n",
    "# print(saved,counter)\n",
    "try:\n",
    "    if counter ==0:\n",
    "        dfs = pd.read_csv(\"pickle_5Mx143.csv\", header=0,   chunksize=chunk)\n",
    " \n",
    "\n",
    "    else:\n",
    "        dfs = pd.read_csv(\"pickle_5Mx143.csv\", skiprows=list(range(1,saved+1)),header=0,   chunksize=chunk)\n",
    "    \n",
    "    \n",
    "    for count, df_chunk in tqdm(enumerate((dfs))):\n",
    "   \n",
    "  \n",
    "            try:\n",
    "                # Alternatively, append the chunk to list and merge all\n",
    "                rows  = str((count+counter)  *chunk+1) +\"_\" +str((count+counter)*chunk+chunk) \n",
    "#                 print(rows)\n",
    "                df_chunk.reset_index(drop=True,inplace=True)\n",
    "\n",
    "                df_chunk.to_feather(\"./data/test_{}.feather\".format(rows))\n",
    "            except:\n",
    "                print('pass')\n",
    "\n",
    "            \n",
    "except:\n",
    "    print('Data is at the end')\n",
    "end = time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 72/72 [00:17<00:00,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.627218913999997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test') & x.lower().endswith('.feather') ]\n",
    "\n",
    "start = time.perf_counter()\n",
    "def read_feather(filename):\n",
    "    return pd.read_feather(f'./data/{filename}')\n",
    "\n",
    "dfs=[]\n",
    "for file in tqdm(files):\n",
    "    dfs.append(read_feather(file))\n",
    "end= time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv(\"test.csv\", nrows=100000 )\n",
    "dfs.to_feather('./data/test.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\enisbe\\\\Documents\\\\conda\\\\gurobi\\\\conda_env\\\\python.exe'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning using Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [03:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185.34035689999996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_el = [0]\n",
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test') & x.lower().endswith('.pickle') ]\n",
    "\n",
    "for file in files:\n",
    "    el = file.split('.')[0].split('test_')[-1].split(\"_\") \n",
    "    for e in el:\n",
    "        try:\n",
    "            new_el.append(int(e))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "saved = max(new_el)\n",
    " \n",
    "saved\n",
    "\n",
    "df_list = [] # list to hold the batch dataframe\n",
    "start = time.perf_counter()\n",
    "\n",
    "chunk = 70000\n",
    "counter = saved//chunk\n",
    "# print(saved,counter)\n",
    "try:\n",
    "    if counter ==0:\n",
    "        dfs = pd.read_csv(\"pickle_5Mx143.csv\", header=0,   chunksize=chunk)\n",
    " \n",
    "\n",
    "    else:\n",
    "        dfs = pd.read_csv(\"pickle_5Mx143.csv\", skiprows=list(range(1,saved+1)),header=0,   chunksize=chunk)\n",
    "    \n",
    "    \n",
    "    for count, df_chunk in tqdm(enumerate((dfs))):\n",
    "   \n",
    "  \n",
    "            try:\n",
    "                # Alternatively, append the chunk to list and merge all\n",
    "                rows  = str((count+counter)  *chunk+1) +\"_\" +str((count+counter)*chunk+chunk) \n",
    "#                 print(rows)\n",
    "                df_chunk.reset_index(drop=True,inplace=True)\n",
    "\n",
    "                df_chunk.to_pickle(\"./data/test_{}.pickle\".format(rows))\n",
    "            except:\n",
    "                print('pass')\n",
    "\n",
    "            \n",
    "except:\n",
    "    print('Data is at the end')\n",
    "end = time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_1050001_1120000.pickle', 'test_1120001_1190000.pickle']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test') & x.lower().endswith('.pickle') ]\n",
    "files[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 72/72 [00:29<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.804165324000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "def read_pickle(filename):\n",
    "    return pd.read_pickle(f'./data/{filename}')\n",
    "\n",
    "dfs=[]\n",
    "for file in tqdm(files):\n",
    "    dfs.append(read_pickle(file))\n",
    "end= time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/source_object_name.pkl', 'wb') as f:\n",
    "    pickle.dump(df_big, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.975307375\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "with open('./data/source_object_name.pkl', 'rb') as f:\n",
    "    big_df = pickle.load( f)\n",
    "    \n",
    "    \n",
    "end= time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.to_pickle(\"./data/big.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.reset_index(drop=True,inplace=True)\n",
    "df_big.to_feather(\"./data/big.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.054833530999996\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "df_big = pd.read_pickle(\"./data/big.pickle\") \n",
    "end= time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big.to_hdf(\"./data/big.hdf5\", key='df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start = time.perf_counter()\n",
    "# df_big = pd.read_hdf(\"./data/big.hdf5\" ) \n",
    "# end= time.perf_counter()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del dfs\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = {}\n",
    "\n",
    "for i in range(10000):\n",
    "    z[i] = np.random.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter multiprocess set up\n",
    "\n",
    "https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pandas as pd \n",
    "import workers.worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_1050001_1120000.feather', 'test_1120001_1190000.feather']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [x for x in os.listdir('./data') if x.lower().startswith('test') & x.lower().endswith('.feather') ]\n",
    "files[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.090229953000005\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "cpu = mp.cpu_count()\n",
    "pool = mp.Pool(8)\n",
    "results = pool.map(workers.worker.read_feather,  files)\n",
    "pool.close()\n",
    "end= time.perf_counter()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 144)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.29336336\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df= workers.worker.read_feather(file)\n",
    "    dfs.append(df)\n",
    "end= time.perf_counter()\n",
    "\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140000, 144)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(results).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
